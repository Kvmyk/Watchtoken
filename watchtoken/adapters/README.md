# Adaptery WatchToken

Ten katalog zawiera adaptery dla r√≥≈ºnych modeli jƒôzykowych. Ka≈ºdy adapter implementuje interfejs `BaseAdapter` i zapewnia specyficznƒÖ dla modelu tokenizacjƒô oraz informacje o kosztach.

## üèóÔ∏è Dostƒôpne adaptery

### ü§ñ OpenAI Adapter (`openai.py`)
- **Modele**: gpt-3.5-turbo, gpt-4, gpt-4-turbo, gpt-4-32k
- **Tokenizer**: tiktoken (precyzyjny)
- **Zale≈ºno≈õci**: `tiktoken>=0.5.0`
- **Status**: ‚úÖ Pe≈Çne wsparcie

```python
# Automatycznie u≈ºywany dla modeli OpenAI
tc = TokenCounter("gpt-4-turbo")
```

### üé≠ Anthropic Adapter (`anthropic.py`) 
- **Modele**: claude-3-sonnet, claude-3-opus, claude-3-haiku
- **Tokenizer**: Estymacja (word + punctuation based)
- **Zale≈ºno≈õci**: Brak
- **Status**: ‚úÖ Estymacja (~95% dok≈Çadno≈õƒá)

```python
tc = TokenCounter("claude-3-sonnet")
```

### üß† Google Adapter (`google.py`)
- **Modele**: gemini-2.5-pro, gemini-2.5-flash, gemini-2.5-flash-lite, gemini-1.5-pro
- **Tokenizer**: Estymacja (character based z adjustmentami)
- **Zale≈ºno≈õci**: Brak  
- **Status**: ‚úÖ Estymacja (~90% dok≈Çadno≈õƒá)

```python
tc = TokenCounter("gemini-2.5-pro")
```

### ‚ö° Mistral Adapter (`mistral.py`)
- **Modele**: mistral-7b, mixtral-8x7b
- **Tokenizer**: SentencePiece (opcjonalnie) lub estymacja
- **Zale≈ºno≈õci**: `sentencepiece>=0.1.99` (opcjonalne)
- **Status**: üöß Beta (estymacja dzia≈Ça, SentencePiece wymaga modelu)

```python
tc = TokenCounter("mistral-7b")
# lub z SentencePiece:
tc = TokenCounter("mistral-7b")  # adapter automatycznie spr√≥buje u≈ºyƒá SP
```

## üîß Tworzenie w≈Çasnego adaptera

### 1. Implementacja BaseAdapter

```python
from watchtoken.adapters import BaseAdapter
from typing import Tuple

class MyCustomAdapter(BaseAdapter):
    def __init__(self, model_name: str) -> None:
        super().__init__(model_name)
        # Inicjalizacja specyficzna dla modelu
    
    def count_tokens(self, text: str) -> int:
        """Zlicz tokeny w tek≈õcie."""
        # Twoja implementacja tokenizacji
        return len(text.split())  # Przyk≈Çad
    
    def get_cost_per_token(self) -> Tuple[float, float]:
        """Zwr√≥ƒá (input_cost, output_cost) per token."""
        return (0.001, 0.002)  # Przyk≈Çad
    
    def get_context_length(self) -> int:
        """Maksymalna d≈Çugo≈õƒá kontekstu."""
        return 4096  # Przyk≈Çad
```

### 2. Rejestracja adaptera

```python
from watchtoken import TokenCounter
from watchtoken.models import ModelConfig, ModelProvider, add_model_config

# Dodaj konfiguracjƒô modelu
add_model_config("my-model", ModelConfig(
    name="my-model",
    provider=ModelProvider.CUSTOM,
    context_length=4096,
    input_cost_per_token=0.001,
    output_cost_per_token=0.002
))

# Zarejestruj adapter
TokenCounter.register_adapter("my-model", MyCustomAdapter)

# U≈ºycie
tc = TokenCounter("my-model")
```

### 3. Przyk≈Çad z zewnƒôtrznym tokenizerem

```python
class HuggingFaceAdapter(BaseAdapter):
    def __init__(self, model_name: str, tokenizer_name: str = None):
        super().__init__(model_name)
        try:
            from transformers import AutoTokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                tokenizer_name or model_name
            )
        except ImportError:
            raise TokenizerError(
                "transformers required: pip install transformers", 
                model_name
            )
    
    def count_tokens(self, text: str) -> int:
        tokens = self.tokenizer.encode(text)
        return len(tokens)
    
    def get_cost_per_token(self) -> Tuple[float, float]:
        # Pobierz z konfiguracji modelu
        config = get_model_config(self.model_name)
        return (config.input_cost_per_token, config.output_cost_per_token)
```

## üìä Dok≈Çadno≈õƒá estymacji

| Adapter | Typ tokenizera | Dok≈Çadno≈õƒá | Notatki |
|---------|---------------|------------|---------|
| OpenAI | tiktoken | ~100% | Oficjalny tokenizer |
| Anthropic | Estymacja | ~95% | Word + punctuation based |
| Google | Estymacja | ~90% | Character based z adjustmentami |
| Mistral | SentencePiece/Est. | ~85-95% | Zale≈ºy od dostƒôpno≈õci modelu SP |

## üîç Debugowanie adapter√≥w

```python
from watchtoken import TokenCounter

tc = TokenCounter("your-model")

# Sprawd≈∫ informacje o modelu
info = tc.get_model_info()
print(f"Model: {info['model']}")
print(f"Provider: {info['provider']}")
print(f"Tokenizer: {info['tokenizer_type']}")

# Test tokenizacji
test_text = "Hello world!"
tokens = tc.count(test_text)
print(f"'{test_text}' -> {tokens} tokens")

# Test koszt√≥w
cost = tc.estimate_cost(test_text, output_tokens=10)
print(f"Cost: ${cost:.6f}")
```

## üöß Przysz≈Çe adaptery

- **LLaMA/Alpaca**: Planowane w v0.2.0
- **PaLM**: Planowane w v0.2.0  
- **Cohere**: Rozwa≈ºane
- **AI21**: Rozwa≈ºane

## ü§ù Wsp√≥≈Çpraca

Chcesz dodaƒá adapter dla nowego modelu? Zobacz [CONTRIBUTING.md](../CONTRIBUTING.md) dla szczeg√≥≈Ç√≥w!

### Checklist dla nowego adaptera:
- [ ] Implementuje `BaseAdapter`
- [ ] Ma testy jednostkowe
- [ ] Dokumentacja w README
- [ ] Obs≈Çuga b≈Çƒôd√≥w z graceful degradation
- [ ] Aktualne ceny w `models.py`
